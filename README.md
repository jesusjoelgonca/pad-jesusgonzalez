
# ğŸ“¦ ExtracciÃ³n Automatizada de Datos con Web Scraping y Despliegue CI/CD

Este proyecto acadÃ©mico tiene como objetivo diseÃ±ar, contenerizar y automatizar un sistema de extracciÃ³n de datos basado en tÃ©cnicas avanzadas de **web scraping**. Se implementan herramientas como **Docker** y **GitHub Actions** para garantizar entornos de ejecuciÃ³n reproducibles y flujos de trabajo automatizados, optimizando asÃ­ los procesos de ingesta y almacenamiento de datos para anÃ¡lisis posteriores.

---

## ğŸ“š DescripciÃ³n General

La iniciativa aborda el reto de recolectar datos estructurados desde plataformas de comercio electrÃ³nico, especÃ­ficamente **MercadoLibre**, utilizando un enfoque combinado de librerÃ­as y frameworks de scraping:

- **BeautifulSoup**: Parsing HTML simple y efectivo.
- **Selenium**: InteracciÃ³n con contenido dinÃ¡mico basado en JavaScript.
- **Scrapy**: Crawling a gran escala y extracciÃ³n eficiente.

Estos scrapers han sido **dockerizados** y se ejecutan automÃ¡ticamente a travÃ©s de un pipeline de **CI/CD** implementado en **GitHub Actions**. El flujo garantiza que cada actualizaciÃ³n de cÃ³digo origine una nueva ejecuciÃ³n de scraping, manteniendo actualizados los conjuntos de datos obtenidos.

---

## ğŸ› ï¸ TecnologÃ­as Utilizadas

- **Python 3.9+**
- **BeautifulSoup**, **Selenium**, **Scrapy**
- **Docker** y **Docker Compose**
- **GitHub Actions** (CI/CD)
- **SQLite3** (persistencia de datos)
- **CSV** (formato de exportaciÃ³n de datos)

---

## ğŸ”§ Estructura del Proyecto

```
â”œâ”€â”€ actividades/
â”‚   â”œâ”€â”€ actividad1/            # AnÃ¡lisis comparativo de herramientas de scraping
â”‚   â”‚   â”œâ”€â”€ informes/           # DocumentaciÃ³n y reportes tÃ©cnicos
â”‚   â”‚   â”œâ”€â”€ static/
â”‚   â”‚   â”‚   â”œâ”€â”€ csv/            # Almacenamiento de datos en CSV
â”‚   â”‚   â”‚   â””â”€â”€ db/             # Almacenamiento de base de datos SQLite
â”‚   â”‚   â””â”€â”€ scripts/            # Scrapers individuales
â”‚   â”œâ”€â”€ actividad3/             # ImplementaciÃ³n de CI/CD con GitHub Actions
â”‚   â”‚   â”œâ”€â”€ workflows/          # ConfiguraciÃ³n de GitHub Actions
â”‚   â”‚   â””â”€â”€ docker/             # Dockerfile y configuraciones de contenerizaciÃ³n
â”œâ”€â”€ docker-compose.yml          # OrquestaciÃ³n de servicios en desarrollo
â”œâ”€â”€ README.md                    # DocumentaciÃ³n principal
```

---

## ğŸš€ Pipeline de CI/CD

El proceso de automatizaciÃ³n estÃ¡ diseÃ±ado para ejecutarse completamente en la nube, asegurando integraciones rÃ¡pidas y despliegues consistentes:

### Flujo de Trabajo

1. **Detonador (Trigger)**: Cada `push` a la rama principal (`main`) inicia el pipeline.
2. **Checkout**: ObtenciÃ³n del cÃ³digo fuente actualizado.
3. **Docker Login**: AutenticaciÃ³n segura en Docker Hub usando secretos gestionados en GitHub.
4. **Build**: ConstrucciÃ³n de imÃ¡genes Docker que contienen los scrapers.
5. **Run**: EjecuciÃ³n automÃ¡tica de los scrapers dentro de contenedores.
6. **Persistencia**:
   - Datos extraÃ­dos guardados en **archivos CSV**.
   - InformaciÃ³n almacenada en **bases de datos SQLite**.

### ConfiguraciÃ³n de GitHub Actions

Archivo principal del workflow:
```yaml
.github/workflows/scraper.yml
```

---

## ğŸ’¾ Persistencia de Datos

Los datos extraÃ­dos son almacenados utilizando volÃºmenes de Docker para asegurar su persistencia entre ejecuciones:

- **CSV**: `actividades/actividad1/static/csv/`
- **SQLite3**: `actividades/actividad1/static/db/`

Estos formatos facilitan su posterior anÃ¡lisis y visualizaciÃ³n en herramientas de procesamiento de datos.

---

## âš™ï¸ Requisitos de InstalaciÃ³n Local

- Docker
- Docker Compose
- Git

### Pasos de InstalaciÃ³n

1. Clonar el repositorio:
   ```bash
   git clone <URL-del-repositorio>
   cd <nombre-del-repositorio>
   ```

2. Construir y ejecutar los servicios:
   ```bash
   docker-compose up --build
   ```

3. Los resultados estarÃ¡n disponibles en las carpetas configuradas para datos.

---

## ğŸ“Š Principales CaracterÃ­sticas

- **Scraping Multi-tÃ©cnica**: IntegraciÃ³n de BeautifulSoup, Selenium y Scrapy para cubrir distintos tipos de pÃ¡ginas web.
- **ContenerizaciÃ³n Completa**: EjecuciÃ³n aislada y replicable de scrapers mediante Docker.
- **AutomatizaciÃ³n Continua**: IntegraciÃ³n y despliegue automÃ¡ticos con GitHub Actions.
- **Persistencia de Alta Fiabilidad**: Almacenamiento estructurado de resultados en CSV y bases de datos relacionales.
- **MonitorizaciÃ³n de Calidad**: Validaciones automÃ¡ticas de integridad de datos.

---

## ğŸ“– DocumentaciÃ³n Relacionada

- [Docker Documentation](https://docs.docker.com/)
- [GitHub Actions Documentation](https://docs.github.com/en/actions)
- [BeautifulSoup Documentation](https://www.crummy.com/software/BeautifulSoup/bs4/doc/)
- [Selenium Documentation](https://www.selenium.dev/documentation/)
- [Scrapy Documentation](https://docs.scrapy.org/en/latest/)
